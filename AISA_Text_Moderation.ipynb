{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AISA-DucHaba/AI-Solution-Architect/blob/main/AISA_Text_Moderation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3WnEac7Fwdgw"
      },
      "source": [
        "# 🌻 Text Moderation\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "- Let's Rock and Roll\n",
        "\n",
        "- **ROLE** defintion by icon:\n",
        "\n",
        "  - 🤠 is AI Solution Architect role.\n",
        "  - 🤖 is AI Scientest role.\n",
        "  - 😎 is Devops role.\n",
        "  - 🤓 is Data Engineer role.\n",
        "  - 🤔 is AI QA role.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DHGKa52qwsp7"
      },
      "source": [
        "## 🤠 Objective\n",
        "\n",
        "**PRIMARY ROLE:** AI Solution Architect\n",
        "\n",
        "- This NLP (Natural Language Processing) AI demonstration aims to prevent profanity, vulgarity, hate speech, violence, sexism, and other offensive language. It is not an act of censorship, as the final UI (User Interface) will give the reader, but not a young reader, the option to click on a label to read the toxic message.\n",
        "\n",
        "- The goal is to create a safer and more respectful environment for you, your colleages, and your family. This NLP app is 1 of 3 hands-on apps from the [\"AI Solution Architect,\"](https://elvtr.com/course/ai-solution-architect?utm_source=instructor&utm_campaign=AISA&utm_content=linkedin) from ELVTR and Duc Haba.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y2GF4K0lcXVp"
      },
      "source": [
        "## 🙈 Legal:\n",
        "\n",
        "---\n",
        "\n",
        "- This Python Jupyter Notebook is for sharing with **Friends** in the AISA course by ELVTR.\n",
        "\n",
        "- If you are **NOT** my friend, and I see you. In the best spirit of the **science community**, you may read this Notebook, but be aware that I see you.\n",
        "\n",
        "- Copyrights 2023 and 2024: [GNU GENERAL PUBLIC LICENSE 3.0](https://www.gnu.org/licenses/gpl-3.0.en.html#license-text)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime\n",
        "\n",
        "# Get the current date and time\n",
        "now = datetime.now()\n",
        "\n",
        "# Format the date and time\n",
        "current_date_time = now.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "\n",
        "# Print the current date and time\n",
        "print(\"Current date and time:\", current_date_time)\n"
      ],
      "metadata": {
        "id": "Tpp017ZnzAkG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BRnCZFMSR0L9"
      },
      "outputs": [],
      "source": [
        "# prompt: print the time\n",
        "\n",
        "# smoke test\n",
        "import datetime\n",
        "print(f'Hello World! Today is: {datetime.datetime.now()}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BUogjldqkhPn"
      },
      "source": [
        "# 😎 Set Up and Verify\n",
        "\n",
        "---\n",
        "\n",
        "- **PRIMARY ROLE:** DevOps Engineer\n",
        "\n",
        "- This section is setting up your environment and verify the server (or laptop) has the correct library and computing power.\n",
        "\n",
        "- I use the Pluto class often in my coding. I created it as an opensource project.\n",
        "\n",
        "- Github: 'https://github.com/duchaba/pluto_happy\n",
        "\n",
        "- Pluto is **optional** for this project. Pluto has a lot of convience functions that use/write otherwise.\n",
        "\n",
        "- Note: Code cell begin with **\"# prompt:\"** is writen by a GenAI, Copilot or Codey.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wFGEfq66P-Hk"
      },
      "source": [
        "## Install required libraries\n",
        "\n",
        "- NOTE ✋: If you run on local laptop or **persistance** platform like, AWS Sagemaker, you need to run pip install just once.\n",
        "\n",
        "- Google Colab is a **non-persistance** platform. Thus you need to install library every time you start up Google Colab.\n",
        "\n",
        "- Hint: The **%%write app.py** is for writing export the code cell for deployment. NOT all code cells are export (because some code are for testing)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pj2BqKWa1Osn"
      },
      "outputs": [],
      "source": [
        "# use capture to hide the long output\n",
        "%%capture log_pip_install_openai\n",
        "\n",
        "!pip install openai\n",
        "!pip install gradio\n",
        "!pip install huggingface_hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U2MogmXySWhj"
      },
      "outputs": [],
      "source": [
        "# If has error, uncomment and print out the log file\n",
        "# print(log_pip_install_openai)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wNsvTFuhJxeX"
      },
      "outputs": [],
      "source": [
        "# %%write app.py\n",
        "# prompt: import openai, huggingface client and gradio\n",
        "\n",
        "import openai\n",
        "import gradio\n",
        "import huggingface_hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sRERRYZpS1Ta"
      },
      "outputs": [],
      "source": [
        "# prompt: print out version of openai, gradio and huggingface_hub\n",
        "\n",
        "print(f\"openai version: {openai.__version__}\")\n",
        "print(f\"gradio version: {gradio.__version__}\")\n",
        "print(f\"huggingface_hub version: {huggingface_hub.__version__}\")\n",
        "\n",
        "# expected values:\n",
        "# openai version: 1.35.14\n",
        "# gradio version: 4.38.1\n",
        "# huggingface_hub version: 0.23.4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6R1i47NZTG2J"
      },
      "source": [
        "- Note: I am using/testing with these version.\n",
        "\n",
        "  - openai version: 1.35.14\n",
        "  - gradio version: 4.38.1\n",
        "  - huggingface_hub version: 0.23.4\n",
        "\n",
        "- You don't have to use the exact version but cognizant of them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iQ4DvPjRT3-I"
      },
      "source": [
        "## 🤠 Install Pluto (Optional)\n",
        "\n",
        "- **PRIMARY ROLE:** AI Solution Arch.\n",
        "\n",
        "- NOTE ✋: Once again **pluto class is optional**. You are free to use other lib or none at all.\n",
        "\n",
        "- It is a base class that everyone in the team should use in their Jupyter Notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r2DeeoKBTvRz"
      },
      "outputs": [],
      "source": [
        "# prompt: print git version\n",
        "\n",
        "!git --version"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1OxFOHNCURTH"
      },
      "source": [
        "- Note: I am using this git version:\n",
        "  - git version 2.34.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KhXCeZTiUihl"
      },
      "outputs": [],
      "source": [
        "# prompt: install lfs and track large file *.pkl\n",
        "\n",
        "# note this optional for git to upload/push large file like the inference engine, *.pkl file\n",
        "!apt -y install git-lfs\n",
        "!git lfs install\n",
        "!git lfs track \"*.pkl\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lopo_0f6nUxH"
      },
      "outputs": [],
      "source": [
        "# prompt: clone https://github.com/duchaba/pluto_happy\n",
        "\n",
        "fname = 'https://github.com/duchaba/pluto_happy'\n",
        "!git clone {fname}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Tn2IezlVhEY"
      },
      "outputs": [],
      "source": [
        "# prompt: list the content of pluto_happy directory\n",
        "\n",
        "!ls -la pluto_happy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WzkHWp-CnU8D"
      },
      "outputs": [],
      "source": [
        "# prompt: pip install requriements.txt\n",
        "\n",
        "%%capture log_pip_install\n",
        "fname = 'pluto_happy/requirements.txt'\n",
        "!pip install -r {fname}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EwaQnDnBxa4F"
      },
      "outputs": [],
      "source": [
        "# print the log file is failed\n",
        "# log_pip_install.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ZeBBZZXojkF"
      },
      "outputs": [],
      "source": [
        "# prompt: run the pluto_happy/pluto.py\n",
        "\n",
        "fname = 'pluto_happy/pluto.py'\n",
        "%run {fname}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yt_Mqx_JWopf"
      },
      "outputs": [],
      "source": [
        "# %%write -a app.py\n",
        "# prompt: create a new class Pluto_Happy and name it monty\n",
        "\n",
        "monty = Pluto_Happy('Monty, Monty Said!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KxuRP5nbojqq"
      },
      "outputs": [],
      "source": [
        "# not_write -a app.py\n",
        "# prompt: None.\n",
        "\n",
        "# print out my environments\n",
        "monty.fname_requirements = 'pluto_happy/requirements.txt'\n",
        "monty.print_info_self()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KlsCH9vMYRS3"
      },
      "source": [
        "- NOTE: Once again, the use of **pluto.print_info_self()** function is for convience. You can write Python code to do the same."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sqpkzBwRbaPF"
      },
      "outputs": [],
      "source": [
        "# prompt: print all monty functions doc\n",
        "\n",
        "help(monty)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pAg9QSv6hMQm"
      },
      "source": [
        "- The documentation is at: https://platform.openai.com/docs/api-reference\n",
        "\n",
        "- https://github.com/openai/openai-python\n",
        "\n",
        "- The Notebook set to GPU and High RAM, but you do not need them to run. It maybe slow without GPU and a lot of RAM, but it will fine.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dYTj6YmpApba"
      },
      "source": [
        "# 🤠 Access to LLM model: STEP 1\n",
        "---\n",
        "\n",
        "- **PRIMARY ROLE:** AI Solution Architect\n",
        "\n",
        "- NOTES: ✋ STOP, define your set of keys before continue\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dLElZNzgMi8l"
      },
      "source": [
        "## Define YOUR Keys: ✋"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J3NPJ7l7nlPF"
      },
      "outputs": [],
      "source": [
        "# # update and uncomment with your key\n",
        "# os.environ['openai_key'] = 'sk-...'\n",
        "# os.environ['huggingface_key'] = 'hf_....'\n",
        "# os.environ['kaggle_key'] = 'daabc...'\n",
        "# os.environ['github_key'] = 'ghp_...'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T3g9l-KGypTA"
      },
      "outputs": [],
      "source": [
        "# Prompt: None:\n",
        "\n",
        "# YOUR KEY GOES HERE...\n",
        "import os\n",
        "import os\n",
        "# os.environ['crypt_key'] = ''\n",
        "# os.environ['openai_key'] = ''\n",
        "# os.environ['github_key'] = ''\n",
        "# os.environ['huggingface_key'] = ''\n",
        "# os.environ['kaggle_key'] = ''\n",
        "# os.environ['google_key'] = ''\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2BgHE64HMCqi"
      },
      "outputs": [],
      "source": [
        "# %%write app.py\n",
        "# Prompt: None\n",
        "# replace the \"getenv()\" with your key string\n",
        "\n",
        "import os\n",
        "monty._openai_key=os.getenv('openai_key')\n",
        "monty._github_key=os.getenv('github_key')\n",
        "monty._huggingface_key=os.getenv('huggingface_key')\n",
        "monty._kaggle_key=os.getenv('kaggle_key')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FgCpuPolnmPY"
      },
      "source": [
        "# 🤖 Access LLM: STEP 2\n",
        "\n",
        "----\n",
        "\n",
        "- **PRIMARY ROLE:** AI Scientist\n",
        "\n",
        "- Smoke test to see if we can access to the chosen model and run it.\n",
        "\n",
        "- We choose OpenAI Moderate model.\n",
        "  - https://platform.openai.com/docs/guides/moderation\n",
        "\n",
        "- update to OpenAI 1.x API\n",
        "  - first create a class openai.OpenAI (a client)\n",
        "  - then do the task"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mSH08CSL96fN"
      },
      "source": [
        "## POC - OpenAI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6dijfYPyiq97"
      },
      "outputs": [],
      "source": [
        "# prompt: create an ai client with openai\n",
        "\n",
        "import openai\n",
        "ai_client = openai.OpenAI(api_key=monty._openai_key)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dPDm1by88qEb"
      },
      "outputs": [],
      "source": [
        "# help(ai_client.moderations.create)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8fLs18n_7AHq"
      },
      "outputs": [],
      "source": [
        "# prompt: None (use same code in the openAI doc site)\n",
        "\n",
        "p = \"I am but a sheep who is lost in the wood.\"\n",
        "tmodel = \"text-moderation-latest\"\n",
        "resp = ai_client.moderations.create(input=p, model=tmodel)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RsQIEmNMjbBL"
      },
      "outputs": [],
      "source": [
        "# prompt: display resp content\n",
        "\n",
        "resp.results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sb-tGagMkCwV"
      },
      "outputs": [],
      "source": [
        "# prompt: parse resp variable\n",
        "\n",
        "# parse the response\n",
        "for idx, cat in enumerate(resp.results[0].categories):\n",
        "  print(f\"{idx} category: {cat}\")\n",
        "print('SCORE:')\n",
        "for idx, cat in enumerate(resp.results[0].category_scores):\n",
        "  print(f\"{idx} category: {cat}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3jNO1cAXNZKr"
      },
      "source": [
        "## POC - Done 💃\n",
        "---\n",
        "\n",
        "- You just prove the heart of the LLM engine is working.\n",
        "\n",
        "- Technically, you can confidently say the project is \"viable.\"\n",
        "\n",
        "- The crucial part POC is done."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pcZ0RUl35fY_"
      },
      "source": [
        "### 🤖 AI Scientist Walk About on OpenAI (Optional)\n",
        "\n",
        "\n",
        "- AI Scientist to explore futher on OpenAI models.\n",
        "\n",
        "- Doc at: https://platform.openai.com/docs/api-reference\n",
        "\n",
        "- Check out Assistants\n",
        "\n",
        "- Use Dall-e to draw image\n",
        "\n",
        "I delete code so it up to you.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0HW7lY4ET7zb"
      },
      "source": [
        "# 🤓 Fetch Dataset from Kaggle: STEP 3\n",
        "\n",
        "- **PRIMARY ROLE:** Data Engineer\n",
        "\n",
        "- Dataset on kaggle: https://www.kaggle.com/datasets/get2jawa/toxic-comments-train\n",
        "\n",
        "- Goals are:\n",
        "  - download\n",
        "  - import to Pandas Dataframe\n",
        "  - clean\n",
        "  - augment\n",
        "  - inspect\n",
        "  - report on biases\n",
        "  - **AND** whatever you need to do to feel confortable with the datasest."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MpCcJs0WfigG"
      },
      "outputs": [],
      "source": [
        "# prompt install opendatasets\n",
        "!pip install opendatasets\n",
        "import opendatasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AndgQlqQuRVC"
      },
      "source": [
        "## Download"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SysYu5XWTADD"
      },
      "outputs": [],
      "source": [
        "# prompt: Write function with inline documentation to download dataset from Kaggle website using opendatasets lib.\n",
        "\n",
        "# I add line @add_method for put the function to pluto (Note: this is optional)\n",
        "@add_method(Pluto_Happy)\n",
        "def fetch_kaggle_dataset(self,dataset_name, path_to_save):\n",
        "\n",
        "  \"\"\"\n",
        "  Downloads a dataset from Kaggle website using opendatasets library.\n",
        "\n",
        "  Args:\n",
        "    dataset_name: (str) The name of the dataset to download.\n",
        "    path_to_save: (str) The path where the dataset will be saved.\n",
        "\n",
        "  Returns:\n",
        "    None\n",
        "  \"\"\"\n",
        "\n",
        "  try:\n",
        "    # Check if the dataset already exists\n",
        "    if os.path.exists(path_to_save):\n",
        "      print(f'Dataset {dataset_name} already exists.')\n",
        "      return\n",
        "\n",
        "    # Download the dataset\n",
        "    print(f'Downloading dataset {dataset_name}...')\n",
        "    opendatasets.download(dataset_name, path_to_save)\n",
        "    print(f'Dataset {dataset_name} downloaded successfully.')\n",
        "\n",
        "  except Exception as e:\n",
        "    print(f'Error downloading dataset {dataset_name}: {e}')\n",
        "  return None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TGbgPXyTtckU"
      },
      "source": [
        "- Note: You need your kaggle username and access token\n",
        "- Please \"join\" the Jigsaw compitition.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LQQXCl-ysrwI"
      },
      "outputs": [],
      "source": [
        "# prompt: use monty.fetch_kaggle_dataset to download https://www.kaggle.com/competitions/jigsaw-toxic-severity-rating\n",
        "\n",
        "fname = 'https://www.kaggle.com/competitions/jigsaw-toxic-severity-rating'\n",
        "monty.fetch_kaggle_dataset(fname,'kaggle')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: print today date and time\n",
        "\n",
        "print(datetime.datetime.now())\n"
      ],
      "metadata": {
        "id": "EqliK30EOm4t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XWgb7GduiGhq"
      },
      "outputs": [],
      "source": [
        "# prompt: list the data in kaggle/jigsaw-toxic-severity-rating\n",
        "!ls -la kaggle/jigsaw-toxic-severity-rating"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6izTs_92uXvG"
      },
      "source": [
        "## Import to Pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zhqTyIFVT6sT"
      },
      "outputs": [],
      "source": [
        "# prompt: load fname csv into dataframe\n",
        "\n",
        "import pandas\n",
        "fname = '/content/kaggle/jigsaw-toxic-severity-rating/validation_data.csv'\n",
        "monty.df_toxic_data = pandas.read_csv(fname)\n",
        "monty.df_toxic_data.head(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "APfUKF2Qut8X"
      },
      "source": [
        "## Clean data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_bquvd8QUy4M"
      },
      "outputs": [],
      "source": [
        "# prompt: replace \\n with space in the df_toxic_data column less_toxic and more_toxic\n",
        "\n",
        "monty.df_toxic_data['less_toxic'] = monty.df_toxic_data['less_toxic'].str.replace('\\n', ' ')\n",
        "monty.df_toxic_data['more_toxic'] = monty.df_toxic_data['more_toxic'].str.replace('\\n', ' ')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Zfyt66nYkxy"
      },
      "outputs": [],
      "source": [
        "# prompt: replace \\n with space in the df_toxic_data column less_toxic and more_toxic\n",
        "\n",
        "monty.df_toxic_data['less_toxic'] = monty.df_toxic_data['less_toxic'].str.replace('http', 'tthp')\n",
        "monty.df_toxic_data['more_toxic'] = monty.df_toxic_data['more_toxic'].str.replace('http', 'tthp')\n",
        "monty.df_toxic_data['less_toxic'] = monty.df_toxic_data['less_toxic'].str.replace('.com', '.no')\n",
        "monty.df_toxic_data['more_toxic'] = monty.df_toxic_data['more_toxic'].str.replace('.com', '.no')\n",
        "monty.df_toxic_data['less_toxic'] = monty.df_toxic_data['less_toxic'].str.replace('.org', '.no')\n",
        "monty.df_toxic_data['more_toxic'] = monty.df_toxic_data['more_toxic'].str.replace('.org', '.no')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ep4Yw4idVrnO"
      },
      "outputs": [],
      "source": [
        "# prompt: replace any non-printing character with space in the df_toxic_data column less_toxic and more_toxic\n",
        "\n",
        "monty.df_toxic_data['less_toxic'] = monty.df_toxic_data['less_toxic'].str.replace('[^\\\\x00-\\\\x7F]', ' ')\n",
        "monty.df_toxic_data['more_toxic'] = monty.df_toxic_data['more_toxic'].str.replace('[^\\\\x00-\\\\x7F]', ' ')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oxs-rW7MVLnz"
      },
      "outputs": [],
      "source": [
        "# prompt: set panda row display to be 250 character long\n",
        "\n",
        "pandas.set_option('display.max_colwidth', 550)\n",
        "monty.df_toxic_data.sample(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HudGAJuLbJxR"
      },
      "outputs": [],
      "source": [
        "monty.df_toxic_data.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aNmhXwKLvmS7"
      },
      "source": [
        "## Save it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E6LKY6-RuhN3"
      },
      "outputs": [],
      "source": [
        "# prompt: write monty.df_toxic_data dataframe to csv file\n",
        "\n",
        "monty.df_toxic_data.to_csv('toxic_data.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CUaMFR8KvCx8"
      },
      "outputs": [],
      "source": [
        "# %%write -a app.py\n",
        "\n",
        "# fname = 'toxic_data.csv'\n",
        "# monty.df_toxic_data = pandas.read_csv(fname)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QCx5vYanSHsJ"
      },
      "source": [
        "# 🤓 Investigate data\n",
        "\n",
        "- **PRIMARY ROLE:** Data engineer\n",
        "- Count average word size of more_toxic.\n",
        "- Count average word size of less_toxic.\n",
        "- Plot histogram\n",
        "- Report statistic\n",
        "- Draw word cloud"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-976yJ46w1RM"
      },
      "source": [
        "## Count words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XoFb2lqESJMN"
      },
      "outputs": [],
      "source": [
        "# prompt: create a new column in monty.df_toxic_data dataframe with the word count from \"less_toxic\" column\n",
        "\n",
        "monty.df_toxic_data['less_toxic_word_count'] = monty.df_toxic_data['less_toxic'].apply(lambda x: len(x.split()))\n",
        "monty.df_toxic_data.head(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3iJOe7l-S0N2"
      },
      "outputs": [],
      "source": [
        "# prompt: create a new column in monty.df_toxic_data dataframe with the word count from \"less_toxic\" column\n",
        "\n",
        "monty.df_toxic_data['more_toxic_word_count'] = monty.df_toxic_data['more_toxic'].apply(lambda x: len(x.split()))\n",
        "monty.df_toxic_data.sample(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jZD2gZ2WS0Q2"
      },
      "outputs": [],
      "source": [
        "# prompt: find the sum of \"less_toxic_word_count\"\n",
        "\n",
        "lcount = monty.df_toxic_data['less_toxic_word_count'].sum()\n",
        "mcount = monty.df_toxic_data['more_toxic_word_count'].sum()\n",
        "print(lcount + mcount)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B7va24DDxriU"
      },
      "source": [
        "## Draw histogram"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AMYaxRFuT5KJ"
      },
      "outputs": [],
      "source": [
        "# prompt: using pandas to draw the histogram of \"less_toxic_word_count\"\n",
        "\n",
        "x = monty.df_toxic_data['less_toxic_word_count'].plot.hist(bins=10,\n",
        "  title='Less Toxic Word Count Histogram')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c8fRmqDXT5N5"
      },
      "outputs": [],
      "source": [
        "x = monty.df_toxic_data['more_toxic_word_count'].plot.hist(bins=10,\n",
        "  title='More Toxic Word Count Histogram')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vkwRj3vlyqXz"
      },
      "source": [
        "## Report statistic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6zQYhB7eS0T_"
      },
      "outputs": [],
      "source": [
        "# prompt: print the max, min, mean, and std of column \"les_toxic_word_count\"\n",
        "\n",
        "max_value = monty.df_toxic_data['less_toxic_word_count'].max()\n",
        "min_value = monty.df_toxic_data['less_toxic_word_count'].min()\n",
        "mean_value = monty.df_toxic_data['less_toxic_word_count'].mean()\n",
        "std_value = monty.df_toxic_data['less_toxic_word_count'].std()\n",
        "\n",
        "print(f\"Max: {max_value}, Min: {min_value}, Mean: {mean_value}, Std: {std_value}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qOfGf2w_SJSF"
      },
      "outputs": [],
      "source": [
        "# prompt: print the max, min, mean, and std of column \"les_toxic_word_count\"\n",
        "\n",
        "max_value = monty.df_toxic_data['more_toxic_word_count'].max()\n",
        "min_value = monty.df_toxic_data['more_toxic_word_count'].min()\n",
        "mean_value = monty.df_toxic_data['more_toxic_word_count'].mean()\n",
        "std_value = monty.df_toxic_data['more_toxic_word_count'].std()\n",
        "\n",
        "print(f\"Max: {max_value}, Min: {min_value}, Mean: {mean_value}, Std: {std_value}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DzsN-1s2Yt8F"
      },
      "outputs": [],
      "source": [
        "import wordcloud\n",
        "# help(wordcloud.WordCloud)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ql9iQZjuy_Hw"
      },
      "source": [
        "## Draw word cloud"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iRvdUVTzSJU8"
      },
      "outputs": [],
      "source": [
        "# prompt: write a Python function with documentation for drawing a word cloud plot for a dataframe 'less_toxic_word_count'.\n",
        "\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def generate_wordcloud(df, column, title=\"Word Cloud\"):\n",
        "    \"\"\"\n",
        "    Generate a word cloud from text data in a specified DataFrame column.\n",
        "\n",
        "    Args:\n",
        "    df (pd.DataFrame): The DataFrame containing the text data.\n",
        "    column (str): The name of the column containing the text data.\n",
        "\n",
        "    Returns:\n",
        "    None\n",
        "    \"\"\"\n",
        "    # Ensure the column exists in the DataFrame\n",
        "    if column not in df.columns:\n",
        "        print(f\"The column {column} does not exist in the DataFrame.\")\n",
        "        return\n",
        "\n",
        "    # Combine all the text from the column into a single string\n",
        "    text = ' '.join(df[column].astype(str).values)\n",
        "\n",
        "    # create special word stops\n",
        "    my_stop_words = {'page', 'will', 'one', 'edit', 'article', 'know', 'way', 'say'}\n",
        "    combined_set = STOPWORDS.union(my_stop_words)\n",
        "\n",
        "    # Create a WordCloud object and generate the wordcloud\n",
        "    wordcloud = WordCloud(background_color='white', width=800, height=800,\n",
        "      max_words=300, stopwords=combined_set).generate(text)\n",
        "\n",
        "    # Display the generated wordcloud\n",
        "    plt.figure(figsize=(8,8),facecolor = None)\n",
        "    plt.imshow(wordcloud)\n",
        "    plt.axis(\"off\")\n",
        "    plt.title(title)\n",
        "    plt.tight_layout(pad=0)\n",
        "    plt.show()\n",
        "\n",
        "    # Save the wordcloud to a file\n",
        "    wordcloud.to_file('wordcloud.png')\n",
        "    return\n",
        "\n",
        "# Example usage:-\n",
        "# generate_wordcloud(result_df, 'Article_Text')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7hL8K0uoW2IM"
      },
      "outputs": [],
      "source": [
        "generate_wordcloud(monty.df_toxic_data, \"less_toxic\", \"Less Toxic Word Cloud\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wubc9JvSW2LJ"
      },
      "outputs": [],
      "source": [
        "generate_wordcloud(monty.df_toxic_data, \"more_toxic\", \"More Toxic Word Cloud\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RGDL-RMQW2YX"
      },
      "outputs": [],
      "source": [
        "# view word not count\n",
        "wordcloud.STOPWORDS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GE-02FKebQpL"
      },
      "outputs": [],
      "source": [
        "my_stop_words = {'page', 'will', 'one', 'edit', 'article', 'know', 'way', 'say'}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RCxGAg8Zca2D"
      },
      "outputs": [],
      "source": [
        "# prompt: combine two sets, wordcloud.STOPWORDS and my_stop_words\n",
        "\n",
        "combined_set = wordcloud.STOPWORDS.union(my_stop_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "usiWUxYtca5R"
      },
      "outputs": [],
      "source": [
        "combined_set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cNBretL5cbZD"
      },
      "outputs": [],
      "source": [
        "# redraw them by re-run previous generate_wordcloud cell/command."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ENILHFEQ3Qdq"
      },
      "source": [
        "# 🤔 Reports\n",
        "\n",
        "- **PRIMARY ROLE:** AI QA engineer\n",
        "\n",
        "- Work with the Data engineer to write a report and potential data biases\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I-TYGDL_AzG7"
      },
      "source": [
        "# 🤖 Write API: Step 4\n",
        "\n",
        "- **PRIMARY ROLE:** AI scientist\n",
        "\n",
        "- Write a few functions to make the API and hook into Gradio and Huggingface."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UpXTPeD_IIFp"
      },
      "outputs": [],
      "source": [
        "# %%writefile -a app.py\n",
        "# prompt: (combine of many seperate prompts and copy code into one code cell)\n",
        "\n",
        "# for openai version 1.3.8\n",
        "@add_method(Pluto_Happy)\n",
        "#\n",
        "def _fetch_moderate_engine(self):\n",
        "  self.ai_client = openai.OpenAI(api_key=self._openai_key)\n",
        "  self.text_model = \"text-moderation-latest\"\n",
        "  return\n",
        "#\n",
        "@add_method(Pluto_Happy)\n",
        "# f\n",
        "def _censor_me(self, p, safer=0.0005):\n",
        "  self._fetch_moderate_engine()\n",
        "  resp_orig = self.ai_client.moderations.create(input=p, model=self.text_model)\n",
        "  resp_dict = resp_orig.model_dump()\n",
        "  #\n",
        "  v1 = resp_dict[\"results\"][0][\"category_scores\"]\n",
        "  v1 = {key: value if value is not None else 0 for key, value in v1.items()}\n",
        "  print(f'resp_dic: {resp_dict}')\n",
        "  print(f'v1: {v1}')\n",
        "\n",
        "  max_key = max(v1, key=v1.get)\n",
        "  max_value = v1[max_key]\n",
        "  sum_value = sum(v1.values())\n",
        "  #\n",
        "  v1[\"is_safer_flagged\"] = False\n",
        "  if (max_value >= safer):\n",
        "    v1[\"is_safer_flagged\"] = True\n",
        "  v1[\"is_flagged\"] = resp_dict[\"results\"][0][\"flagged\"]\n",
        "  v1['max_key'] = max_key\n",
        "  v1['max_value'] = max_value\n",
        "  v1['sum_value'] = sum_value\n",
        "  v1['safer_value'] = safer\n",
        "  v1['message'] = p\n",
        "  return v1\n",
        "#\n",
        "@add_method(Pluto_Happy)\n",
        "def _draw_censor(self,data):\n",
        "  self._color_mid_gray = '#6c757d'\n",
        "  exp = (0.01, 0.01)\n",
        "  x = [data['max_value'], (1-data['max_value'])]\n",
        "  title=f\"\\nUnsafe: {data['max_key']}: {(data['max_value']*100):.2f}% Confidence\\n\"\n",
        "  lab = [data['max_key'], 'Other 13 categories']\n",
        "  if (data['is_flagged']):\n",
        "    col=[self.color_danger, self.color_mid_gray]\n",
        "  elif (data['is_safer_flagged']):\n",
        "    col=[self.color_warning, self.color_mid_gray]\n",
        "    lab = ['Relative Score:\\n'+data['max_key'], 'Other 13 categories']\n",
        "    title=f\"\\nPersonal Unsafe: {data['max_key']}: {(data['max_value']*100):.2f}% Confidence\\n\"\n",
        "  else:\n",
        "    col=[self.color_mid_gray, self.color_success]\n",
        "    lab = ['False Negative:\\n'+data['max_key'], 'Other 13 categories']\n",
        "    title='\\nSafe Message\\n'\n",
        "  canvas = self._draw_donut(x, lab, col, exp,title)\n",
        "  return canvas\n",
        "#\n",
        "@add_method(Pluto_Happy)\n",
        "def _draw_donut(self,data,labels,col, exp,title):\n",
        "  # col = [self.color_danger, self._color_secondary]\n",
        "  # exp = (0.01, 0.01)\n",
        "  # Create a pie chart\n",
        "  canvas, pic = matplotlib.pyplot.subplots()\n",
        "  pic.pie(data, explode=exp,\n",
        "    labels=labels,\n",
        "    colors=col,\n",
        "    autopct='%1.1f%%',\n",
        "    startangle=90,\n",
        "    textprops={'color':'#0a0a0a'})\n",
        "  # Draw a circle at the center of pie to make it look like a donut\n",
        "  # centre_circle = matplotlib.pyplot.Circle((0,0),0.45,fc='white')\n",
        "  centre_circle = matplotlib.pyplot.Circle((0,0),0.45,fc=col[0],linewidth=2, ec='white')\n",
        "  canvas = matplotlib.pyplot.gcf()\n",
        "  canvas.gca().add_artist(centre_circle)\n",
        "\n",
        "  # Equal aspect ratio ensures that pie is drawn as a circle.\n",
        "  pic.axis('equal')\n",
        "  pic.set_title(title)\n",
        "  canvas.tight_layout()\n",
        "  # canvas.show()\n",
        "  return canvas\n",
        "#\n",
        "@add_method(Pluto_Happy)\n",
        "# def censor_me(self, msg, safer=0.02, ibutton_1=0):\n",
        "def fetch_toxicity_level(self, msg, safer):\n",
        "  # safer=0.2\n",
        "  yjson = self._censor_me(msg,safer)\n",
        "  _canvas = self._draw_censor(yjson)\n",
        "  _yjson = json.dumps(yjson, indent=4)\n",
        "  return (_canvas, _yjson)\n",
        "  #return(_canvas)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "50UWtl4KPhCA"
      },
      "outputs": [],
      "source": [
        "# help(matplotlib.pyplot.Circle)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JC2DivZF4grS"
      },
      "source": [
        "## 🤔 Smoke test the functions\n",
        "\n",
        "- **PRIMARY ROLE:** AI QA engineer\n",
        "\n",
        "- QA all the functions above"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "djysBd841sAa"
      },
      "outputs": [],
      "source": [
        "# prompt: none\n",
        "# Smoke test\n",
        "resp = monty._censor_me(\"I am but a sheep who is lost in the wood.\")\n",
        "resp"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "donut, jreturn = monty.fetch_toxicity_level(\"I am but a sheep who is lost in the wood.\", 0.02)"
      ],
      "metadata": {
        "id": "9wbfl2Uft3f9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(jreturn)"
      ],
      "metadata": {
        "id": "Omu7T24nuGAr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ABOp6bgXcTSy"
      },
      "outputs": [],
      "source": [
        "# # prompt: print the first value of monty.df_toxic_data['more_toxic']\n",
        "\n",
        "# monty.df_toxic_data['more_toxic'].values[0]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DFtR4kkHa041"
      },
      "outputs": [],
      "source": [
        "# prompt: none\n",
        "\n",
        "# test with kaggle data\n",
        "msg = str(monty.df_toxic_data['more_toxic'].sample(1).values[0])\n",
        "resp = monty._censor_me(msg)\n",
        "resp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LIGrnIDJ2s7w"
      },
      "outputs": [],
      "source": [
        "# prompt: none\n",
        "\n",
        "# again: test with kaggle data\n",
        "msg = str(monty.df_toxic_data['more_toxic'].sample(1).values[0])\n",
        "resp = monty._censor_me(msg)\n",
        "resp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IqhAH3Ok2xjq"
      },
      "outputs": [],
      "source": [
        "# prompt: none\n",
        "\n",
        "# again: test with kaggle data\n",
        "msg = str(monty.df_toxic_data['more_toxic'].sample(1).values[0])\n",
        "resp = monty._censor_me(msg)\n",
        "resp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rqiP3ho3JbKW"
      },
      "source": [
        "# 🤖 Define HuggingFace Gradio Interface\n",
        "\n",
        "- PRIMARY ROLE: AI scientist\n",
        "\n",
        "- Build from scratch using Blocks() instead of the short cut using Interface()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R9XhrCVWuP7d"
      },
      "outputs": [],
      "source": [
        "# %%write -a app.py\n",
        "# prompt: result from a lot of prompt AI and old fashion try and error\n",
        "\n",
        "import random\n",
        "def say_hello(val):\n",
        "  return f\"Hello: {val}\"\n",
        "def say_toxic():\n",
        "  return f\"I am toxic\"\n",
        "def fetch_toxic_tweets(maxi=2):\n",
        "    sample_df = monty.df_toxic_data.sample(maxi)\n",
        "    is_true = random.choice([True, False])\n",
        "    c1 = \"more_toxic\"\n",
        "    if is_true:\n",
        "      c1 = \"less_toxic\"\n",
        "    toxic1 = sample_df[c1].iloc[0]\n",
        "    # toxic1 = \"cat eats my homework.\"\n",
        "    return sample_df.to_html(index=False), toxic1\n",
        "#\n",
        "# define all gradio widget/components outside the block for easy to visualize the blocks structure\n",
        "#\n",
        "in1 = gradio.Textbox(lines=3, label=\"Enter Text:\")\n",
        "in2 = gradio.Slider(0.005, .1, value=0.02, step=.005,label=\"Personalize Safer Value: (larger value is less safe)\")\n",
        "out1 = gradio.Plot(label=\"Output:\")\n",
        "out2 = gradio.HTML(label=\"Real-world Toxic Posts/Tweets: *WARNING\")\n",
        "out3 = gradio.Textbox(lines=5, label=\"Output JSON:\")\n",
        "but1 = gradio.Button(\"Measure 14 Toxicity\", variant=\"primary\",size=\"sm\")\n",
        "but2 = gradio.Button(\"Fetch Toxic Text\", variant=\"stop\", size=\"sm\")\n",
        "#\n",
        "txt1 = \"\"\"\n",
        "# 😃 Welcome To The Friendly Text Moderation\n",
        "\n",
        "### Identify 14 categories of text toxicity.\n",
        "\n",
        "> This NLP (Natural Language Processing) AI demonstration aims to prevent profanity, vulgarity, hate speech, violence, sexism, and other offensive language.\n",
        ">It is **not an act of censorship**, as the final UI (User Interface) will give the reader, but not a young reader, the option to click on a label to read the toxic message.\n",
        ">The goal is to create a safer and more respectful environment for you, your colleages, and your family.\n",
        "> This NLP app is 1 of 3 hands-on courses, [\"AI Solution Architect,\" from ELVTR and Duc Haba](https://elvtr.com/course/ai-solution-architect?utm_source=instructor&utm_campaign=AISA&utm_content=linkedin).\n",
        "---\n",
        "### 🌴 Helpful Instruction:\n",
        "\n",
        "1. Enter your [harmful] message in the input box.\n",
        "\n",
        "2. Click the \"Measure 14 Toxicity\" button.\n",
        "3. View the result on the Donut plot.\n",
        "4. (**Optional**) Click on the \"Fetch Real World Toxic Dataset\" below.\n",
        "5. There are additional options and notes below.\n",
        "\"\"\"\n",
        "txt2 = \"\"\"\n",
        "## 🌻 Author and Developer Notes:\n",
        "---\n",
        "- The demo uses the cutting-edge (2024) AI Natural Language Processing (NLP) model from OpenAI.\n",
        "- This NLP app is 1 of 3 hands-on apps from the [\"AI Solution Architect,\" from ELVTR and Duc Haba](https://elvtr.com/course/ai-solution-architect?utm_source=instructor&utm_campaign=AISA&utm_content=linkedin).\n",
        "\n",
        "- It is not a Generative (GenAI) model, such as Google Gemini or GPT-4.\n",
        "- The NLP understands the message context, nuance, innuendo, and not just swear words.\n",
        "- We **challenge you** to trick it, i.e., write a toxic tweet or post, but our AI thinks it is safe. If you win, please send us your message.\n",
        "- The 14 toxicity categories are as follows:\n",
        "\n",
        "    1. harassment\n",
        "    2. harassment threatening\n",
        "    3. harassment instructions\n",
        "    4. hate\n",
        "    5. hate threatening\n",
        "    6. hate instructions\n",
        "    7. self harm\n",
        "    8. self harm instructions\n",
        "    9. self harm intent\n",
        "    10. self harm minor\n",
        "    11. sexual\n",
        "    12. sexual minors\n",
        "    13. violence\n",
        "    14. violence graphic\n",
        "\n",
        "- If the NLP model classifies the message as \"safe,\" you can still limit the level of toxicity by using the \"Personal Safe\" slider.\n",
        "- The smaller the personal-safe value, the stricter the limitation. It means that if you're a young or sensitive adult, you should choose a lower personal-safe value, less than 0.02, to ensure you're not exposed to harmful content.\n",
        "- The color of the donut plot is as follows:\n",
        "  - Red is an \"unsafe\" message by the NLP model\n",
        "  - Green is a \"safe\" message\n",
        "  - Yellow is an \"unsafe\" message by your toxicity level\n",
        "\n",
        "- The **\"confidence\"** score refers to the confidence level in detecting a particular type of toxicity among the 14 tracked types. For instance, if the confidence score is 90%, it indicates a 90% chance that the toxicity detected is of that particular type. In comparison, the remaining 13 toxicities collectively have a 10% chance of being the detected toxicity. Conversely, if the confidence score is 3%, it could indicate any toxicity. It's worth noting that the Red, Green, or Yellow safety levels do not influence the confidence score.\n",
        "\n",
        "- The real-world dataset is from the Jigsaw Rate Severity of Toxic Comments on Kaggle. It has 30,108 records.\n",
        "    - Citation:\n",
        "    - Ian Kivlichan, Jeffrey Sorensen, Lucas Dixon, Lucy Vasserman, Meghan Graham, Tin Acosta, Walter Reade. (2021). Jigsaw Rate Severity of Toxic Comments . Kaggle. https://kaggle.com/competitions/jigsaw-toxic-severity-rating\n",
        "- The intent is to share with Duc's friends and colleagues, but for those with nefarious intent, this Text Moderation model is governed by the GNU 3.0 License: https://www.gnu.org/licenses/gpl-3.0.en.html\n",
        "- Author: Copyright (C), 2024 **[Duc Haba](https://linkedin.com/in/duchaba)**\n",
        "---\n",
        "# 🌟 \"AI Solution Architect\" Course by ELVTR\n",
        "\n",
        ">Welcome to the fascinating world of AI and natural language processing (NLP). This NLP model is a part of one of three hands-on application. In our journey together, we will explore the [AI Solution Architect](https://elvtr.com/course/ai-solution-architect?utm_source=instructor&utm_campaign=AISA&utm_content=linkedin) course, meticulously crafted by ELVTR in collaboration with Duc Haba. This course is intended to serve as your gateway into the dynamic and constantly evolving field of AI Solution Architect, providing you with a comprehensive understanding of its complexities and applications.\n",
        "\n",
        ">An AI Solution Architect (AISA) is a mastermind who possesses a deep understanding of the complex technicalities of AI and knows how to creatively integrate them into real-world solutions. They bridge the gap between theoretical AI models and practical, effective applications. AISA works as a strategist to design AI systems that align with business objectives and technical requirements. They delve into algorithms, data structures, and computational theories to translate them into tangible, impactful AI solutions that have the potential to revolutionize industries.\n",
        "\n",
        "> 🍎 [Sign up for the course today](https://elvtr.com/course/ai-solution-architect?utm_source=instructor&utm_campaign=AISA&utm_content=linkedin), and I will see you in class.\n",
        "\n",
        "- An article about this NLP Text Moderation will be coming soon.\n",
        "\"\"\"\n",
        "txt3 = \"\"\"\n",
        "## 💥 WARNING: WARNING:\n",
        "---\n",
        "\n",
        "- The following button will retrieve **real-world** offensive posts from Twitter and customer reviews from consumer companies.\n",
        "- The button will display four toxic messages at a time. **Click again** for four more randomly selected postings/tweets.\n",
        "- They contain **profanity, vulgarity, hate, violence, sexism, and other offensive language.**\n",
        "- After you fetch the toxic messages, Click on the **\"Measure 14 Toxicity\" button**.\n",
        "\"\"\"\n",
        "#reverse_button.click(process_text, inputs=text_input, outputs=reversed_text)\n",
        "#\n",
        "\n",
        "with gradio.Blocks() as gradio_app:\n",
        "  # title\n",
        " gradioMarkdown(txt1) # any html or simple mark up\n",
        "  #\n",
        "  # first row, has two columns 1/3 size and 2/3 size\n",
        "  with gradio.Row():    # items inside rows are columns\n",
        "    # left column\n",
        "    with gradio.Column(scale=1): # items under columns are row, scale is 1/3 size\n",
        "      # left column has two rows, text entry, and buttons\n",
        "      in1.render()\n",
        "      in2.render()\n",
        "      but1.render()\n",
        "      out3.render()\n",
        "      but1.click(monty.fetch_toxicity_level, inputs=[in1, in2], outputs=[out1,out3])\n",
        "\n",
        "    with gradio.Column(scale=2):\n",
        "      out1.render()\n",
        "  #\n",
        "  # second row is warning text\n",
        "  with gradio.Row():\n",
        "    gradio.Markdown(txt3)\n",
        "\n",
        "  # third row is fetching toxic data\n",
        "  with gradio.Row():\n",
        "    with gradio.Column(scale=1):\n",
        "      but2.render()\n",
        "      but2.click(fetch_toxic_tweets, inputs=None, outputs=[out2, in1])\n",
        "    with gradio.Column(scale=2):\n",
        "      out2.render()\n",
        "\n",
        "  # fourth row is note text\n",
        "  with gradio.Row():\n",
        "    gradio.Markdown(txt2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GzCOApzwIC7V"
      },
      "source": [
        "# 🤔 QA - Test it locally on Jupyter Notebook: STEP 5\n",
        "\n",
        "- It will failed to test locally if you running VPN\n",
        "\n",
        "- You should see the app run locally/here as if it is on HuggingFace.\n",
        " - https://huggingface.co/spaces/duchaba/Friendly_Text_Moderation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Adttj_MSuP_y"
      },
      "outputs": [],
      "source": [
        "# %%write -a app.py\n",
        "# prompt: start graido_app\n",
        "\n",
        "#gradio_app.launch(debug=True)\n",
        "gradio_app.launch()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CwC_rWcNPkVn"
      },
      "source": [
        "# 🤠 Presentation and Review\n",
        "\n",
        "- **PRIMARY ROLE:** AI solution architect\n",
        "- As an AI solution architect, you will review the work above.\n",
        "\n",
        "- The rest of the steps are for DevOps"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3YHQsNl05P21"
      },
      "source": [
        "# 😎 Production Deployment\n",
        "\n",
        "- *Duc: I have not clean up this section yet.*\n",
        "\n",
        "- **PRIMARY ROLE:** DevOps engineer\n",
        "\n",
        "- We choose Huggingface website, but in realword it would on AWS or Google Serverless engine or MS Azure servers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "anKLQ3g8vlZm"
      },
      "source": [
        "## Write/create required files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SDo1cXLVzVNF"
      },
      "outputs": [],
      "source": [
        "@add_method(Pluto_Happy)\n",
        "def fetch_code_cells(self, notebook_name,\n",
        "  filter_magic=\"# %%write\",\n",
        "  write_to_file=True, fname_override=None):\n",
        "\n",
        "  \"\"\"\n",
        "  Reads a Jupyter notebook (.ipynb file) and writes out all the code cells\n",
        "  that start with the specified magic command to a .py file.\n",
        "\n",
        "  Parameters:\n",
        "  - notebook_name (str): Name of the notebook file (with .ipynb extension).\n",
        "  - filter_magic (str): Magic command filter. Only cells starting with this command will be written.\n",
        "      The defualt is: \"# %%write\"\n",
        "  - write_to_file (bool): If True, writes the filtered cells to a .py file.\n",
        "      Otherwise, prints them to the standard output. The default is True.\n",
        "  - fname_override (str): If provided, overrides the output filename. The default is None.\n",
        "\n",
        "  Returns:\n",
        "  - None: Writes the filtered code cells to a .py file or prints them based on the parameters.\n",
        "\n",
        "  \"\"\"\n",
        "  with open(notebook_name, 'r', encoding='utf-8') as f:\n",
        "    notebook_content = json.load(f)\n",
        "\n",
        "  output_content = []\n",
        "\n",
        "  # Loop through all the cells in the notebook\n",
        "  for cell in notebook_content['cells']:\n",
        "    # Check if the cell type is 'code' and starts with the specified magic command\n",
        "    if cell['cell_type'] == 'code' and cell['source'] and cell['source'][0].startswith(filter_magic):\n",
        "      # Append the source code of the cell to output_content\n",
        "      output_content.append(''.join(cell['source']))\n",
        "\n",
        "  if write_to_file:\n",
        "    if fname_override is None:\n",
        "      # Derive the output filename by replacing .ipynb with .py\n",
        "      output_filename = notebook_name.replace(\".ipynb\", \".py\")\n",
        "    else:\n",
        "      output_filename = fname_override\n",
        "    with open(output_filename, 'w', encoding='utf-8') as f:\n",
        "      f.write('\\n'.join(output_content))\n",
        "    print(f'File: {output_filename} written to disk.')\n",
        "  else:\n",
        "    # Print the code cells to the standard output\n",
        "    print('\\n'.join(output_content))\n",
        "    print('-' * 40)  # print separator\n",
        "  return\n",
        "# Example usage:\n",
        "# print_code_cells_from_notebook('your_notebook_name_here.ipynb')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fE3_ezCcKNjR"
      },
      "outputs": [],
      "source": [
        "# define the huggingface name\n",
        "monty.hface_space = 'duchaba/Friendly_Text_Moderation'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xn_RtPBoJnIG"
      },
      "outputs": [],
      "source": [
        "# openai: 0.27.7,              Actual: 0.27.7\n",
        "# huggingface_hub: 0.14.1,     Actual: 0.15.1\n",
        "# gradio: 3.32.0,              Actual: 3.32.0\n",
        "# cryptography: 40.0.2,        Actual: 40.0.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dh_AKuSj0tq_"
      },
      "outputs": [],
      "source": [
        "!cat \"/content/pluto_happy/requirements.txt\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N6r8821ktW9Y"
      },
      "outputs": [],
      "source": [
        "# create the requirements.txt file\n",
        "txt = [\"openai\", \"gradio\",\"cryptography\", \"huggingface_hub\", \"psutil\", \"pynvml\", \"py-cpuinfo\", \"flopth\"]\n",
        "monty.write_file(\"requirements.txt\", txt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pSm8cahGvFxI"
      },
      "outputs": [],
      "source": [
        "# optional double check it\n",
        "!cat requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5KjPwdG6X1Gz"
      },
      "source": [
        "**STOP**\n",
        "\n",
        "1. Download this notebook\n",
        "\n",
        "1. Upload it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XodnoxcL1m-9"
      },
      "outputs": [],
      "source": [
        "!pwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C1yKXHVnXCh7"
      },
      "outputs": [],
      "source": [
        "fname = \"/content/AISA_Text_Moderation.ipynb\"\n",
        "monty.fetch_code_cells(fname, fname_override=\"app_part2.py\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_x-1VYWK2jRD"
      },
      "outputs": [],
      "source": [
        "# prompt: use unix command to concat file1 and file2\n",
        "\n",
        "!cat /content/pluto_happy/pluto.py app_part2.py > app.py\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HDS8KtUEtXAo"
      },
      "outputs": [],
      "source": [
        "# uncomment the %%write code cell about to create app.py\n",
        "# then double check it\n",
        "!cat app.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TsF4xLdREBx8"
      },
      "source": [
        "## Create the HuggingFace page\n",
        "\n",
        "- Choose a unique file-space, like happy_butterfly\n",
        "\n",
        "- First option, do it on huggingface.com website (recomented)\n",
        "\n",
        "- Second option, do it programatically (optional, uncomment below)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dulzFow1AT68"
      },
      "outputs": [],
      "source": [
        "# # second option\n",
        "# api = huggingface_hub.HfApi()\n",
        "# api.create_repo(repo_id=pluto.hface_name, private=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mKF1nWC3MRJO"
      },
      "source": [
        "# 🤔 Deploy to HuggingFace Sandbox\n",
        "\n",
        "---\n",
        "\n",
        "- Read the tutorial above if you are confused.\n",
        "\n",
        "- It is easy. \"app.py\" and \"requirements.txt\" are the two files that you need to upload.\n",
        "  - Link to create the app.py file on huggingface web: https://huggingface.co/spaces/duchaba/new/main?filename=app.py\n",
        "\n",
        "- Optional are depending on your more fancy layout/output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BPQFNwRXv80u"
      },
      "source": [
        "## Push to files to Hugging face"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kHfqwn6h7Bgl"
      },
      "outputs": [],
      "source": [
        "@add_method(Pluto_Happy)\n",
        "def _login_hface(self):\n",
        "  huggingface_hub.login(self._huggingface_key,\n",
        "    add_to_git_credential=True) # non-blocking login\n",
        "  self._ph()\n",
        "  return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mGWpv6Sc46Tn"
      },
      "outputs": [],
      "source": [
        "monty._login_hface()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Qc9s4jO7V7V"
      },
      "outputs": [],
      "source": [
        "monty.hface_space"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yWkJsiH9AT-N"
      },
      "outputs": [],
      "source": [
        "up_files = [\"app.py\", \"requirements.txt\", \"toxic_data.csv\"]\n",
        "monty.push_hface_files(up_files, hf_space=monty.hface_space)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tXzSFXj-0reI"
      },
      "outputs": [],
      "source": [
        "# test it\n",
        "print(f'https://huggingface.co/spaces/{monty.hface_space}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P0BqlE3dGbnq"
      },
      "source": [
        "# 😎 Pull and Push to Github (Optional)\n",
        "\n",
        "- **PRIMARY ROLE:** DevOps engineer\n",
        "\n",
        "- *Duc: I have not clean up this section yet.*\n",
        "\n",
        "**Note:** ✋\n",
        "\n",
        "- QA it on this notebook **BEFORE** push it.\n",
        "\n",
        "- If you change any data or files, commit and push it to github. For now, we don't need pull-request, so push it to main or your-branch-name.\n",
        "\n",
        "- I ussualy comment out the section because I don't want to accidental run it (when not ready)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MIVSHSwmGbnq"
      },
      "outputs": [],
      "source": [
        "!pwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kxPpK4xmGKDX"
      },
      "outputs": [],
      "source": [
        "fname = 'https://github.com/AISA-DucHaba/AI-Solution-Architect.git'\n",
        "!git clone {fname}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iytF4it5eCt3"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "f = '/content/AI-Solution-Architect'\n",
        "os.chdir(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X-nh-RICGpaj"
      },
      "outputs": [],
      "source": [
        "!pwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lBESQ2AAGbnq"
      },
      "outputs": [],
      "source": [
        "!git branch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FlvaEHrEGbnr"
      },
      "outputs": [],
      "source": [
        "!git pull"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jKnWWmInGbnr"
      },
      "outputs": [],
      "source": [
        "# check for update file\n",
        "!git diff --name-only"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Nonybh2NGP_"
      },
      "outputs": [],
      "source": [
        "# prompt: show the github diff in previous version\n",
        "\n",
        "!git diff HEAD^ HEAD\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uWTzuEkld8E6"
      },
      "outputs": [],
      "source": [
        "# import os\n",
        "# f = 'Data-Augmentation-with-Python'\n",
        "# os.chdir(f)\n",
        "!git add -A\n",
        "!git config --global user.email \"duc.haba@gmail.com\"\n",
        "!git config --global user.name \"duchaba\"\n",
        "!git commit -m \"add raw data files from kaggle to data directory\"\n",
        "# # do the git push in the xterm console\n",
        "# #!git push"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XMDuDF5gGbnr"
      },
      "outputs": [],
      "source": [
        "# # check for any in stage ready to commit\n",
        "# !git diff --name-only --staged"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pAIl7ZzEGbns"
      },
      "outputs": [],
      "source": [
        "# check what were the commits\n",
        "!git log --name-status HEAD^..HEAD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Azo-O8pGbnt"
      },
      "outputs": [],
      "source": [
        "# double check it before commit\n",
        "!git status"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dmPsmXZBuaB3"
      },
      "outputs": [],
      "source": [
        "# !pip install lfs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fU6cc4f4ulGZ"
      },
      "outputs": [],
      "source": [
        "# prompt: git push large file\n",
        "\n",
        "# !git-lfs track *.pkl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dWaoO8u2Gbnt"
      },
      "outputs": [],
      "source": [
        "# push it\n",
        "fname = 'https://duchaba:@github.com/AISA-DucHaba/AI-Solution-Architect.git'\n",
        "!git push {fname}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NDRDuZL6Mnvt"
      },
      "outputs": [],
      "source": [
        "# !curl https://api.openai.com/v1/moderations \\\n",
        "#   -H \"Content-Type: application/json\" \\\n",
        "#   -H \"Authorization: Bearer $OPENAI_API_KEY\" \\\n",
        "#   -d '{\"input\": \"I want to kill them all.\"}'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9wsZtfaqTYAN"
      },
      "outputs": [],
      "source": [
        "# prompt: zip a directory data\n",
        "fname = '/content/AI-Solution-Architect/data'\n",
        "!zip -r data.zip {fname}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oDkCp0WxTpQK"
      },
      "outputs": [],
      "source": [
        "# prompt: download /content/AI-Solution-Architect/data.zip file\n",
        "\n",
        "from google.colab import files\n",
        "files.download('/content/AI-Solution-Architect/data.zip')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P_jLD0Z7AUKk"
      },
      "source": [
        "# That's it. It's dancing time: 🕺"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pUzE7MfOI1Sl"
      },
      "outputs": [],
      "source": [
        "print('the end.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9VL5egEhI16n"
      },
      "outputs": [],
      "source": [
        "# monty.print_dancing()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N2x-NQlhNSD9"
      },
      "source": [
        "# 🤠 Conclusion\n",
        "\n",
        "- That is it for the NLP text moderation from soup to nuts.\n",
        "\n",
        "- Use this LLM \"as-is\" or as a template to create your own model."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "v1 = {'harassment': 0.4044782519340515, 'harassment_threatening': 0.2582634687423706, 'hate': 0.00500369630753994, 'hate_threatening': 0.0005717452731914818, 'illicit': None, 'illicit_violent': None, 'self_harm': 0.0004080208018422127, 'self_harm_instructions': 1.6580557712586597e-05, 'self_harm_intent': 0.00043149717384949327, 'sexual': 0.0015329490415751934, 'sexual_minors': 6.829887570347637e-05, 'violence': 0.816072404384613, 'violence_graphic': 0.0012203836813569069, 'self-harm': 0.0004080208018422127, 'sexual/minors': 6.829887570347637e-05, 'hate/threatening': 0.0005717452731914818, 'violence/graphic': 0.0012203836813569069, 'self-harm/intent': 0.00043149717384949327, 'self-harm/instructions': 1.6580557712586597e-05, 'harassment/threatening': 0.2582634687423706}"
      ],
      "metadata": {
        "id": "6vkge-9YX18b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "v2 = {key: value if value is not None else 0 for key, value in v1.items()}\n",
        "v2.values()"
      ],
      "metadata": {
        "id": "NIs9gqg_ZCrb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sum_value = sum(value for value in v1.values() if value is not None)\n",
        "sum_value"
      ],
      "metadata": {
        "id": "fw-_zG0ZX_uV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "v1.values()"
      ],
      "metadata": {
        "id": "Pgv9ExwDX_xh"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPXTmcBA4wfNy5NWIxTKCon",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}